{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DES Y3 2x2pt Likelihood for LCDM Model v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: BayesFast is still undocumented and under development by He Jia at https://github.com/HerculesJack/bayesfast. James Sullivan (jmsullivan@berkeley.edu) modified this notebook for Y3 2x2pt.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the [dask](https://distributed.dask.org/en/latest/) client. \n",
    "Here we have one node with 64 cores. \n",
    "See [this](https://jobqueue.dask.org/en/latest/configurations.html#nersc-cori)\n",
    "for examples on how to start the client using multiple nodes on NERSC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client, LocalCluster\n",
    "cluster = LocalCluster(n_workers=64, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Inputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your chain here if you want to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#if you have a chain you want to check, put it here and set to True\n",
    "chain_supplied=False\n",
    "your_chain_path = ' '\n",
    "\n",
    "#get number of parameters and comparison chain if necessary\n",
    "if(not chain_supplied):\n",
    "    #example multinest chain - only used to get number of parameters if you don't supply a chain\n",
    "    cfile ='./y3_notebook_v1/mn_inifiles/chain_2x2pt_fiducial_scale_cut_test_nl_bias_baryon.fits_scales_2x2pt_12_6.ini_lcdm.txt'\n",
    "    nParams = np.loadtxt(cfile).shape[1]-5\n",
    "else:\n",
    "    your_chain = np.loadtxt(your_chain_path)\n",
    "    nParams = your_chain.shape[1]-5\n",
    "    chain_x_mn,chain_p_mn = your_chain[:,:nParams].copy(),your_chain[:, -1].copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide whether to use importance sampling, and if so how many samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useIS=True\n",
    "n_IS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading ini files:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the parameter ranges and start values for the parameters which are not fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file reading to get the priors, ranges, and init values \n",
    "priors,values=[],[]\n",
    "pkeys,vkeys=[],[]\n",
    "\n",
    "def is_number(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "with open('./values.ini') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        if('=' in line and '#' not in line and ';' not in line):\n",
    "            line_values=[]\n",
    "            if(i<=4 ): #\\t is the separator instead of space for some reason\n",
    "                pre_num_list = line[:-1].split('=')[1].split('\\t')\n",
    "            else:\n",
    "                pre_num_list = line[:-1].split('=')[1].split(' ')\n",
    "            for substr in pre_num_list: \n",
    "                if(is_number(substr)): \n",
    "                    line_values.append(float(substr))\n",
    "            if(len(line_values)>1): #ignore fixed parameters\n",
    "                if('alpha1' in line or 'alpha2' in line): #spacing is different for alpha\n",
    "                    vkeys.append(line.split('=')[0])\n",
    "                else:\n",
    "                    vkeys.append(line.split(' ')[0]) \n",
    "                values.append(line_values)\n",
    "                        \n",
    "#all gaussian\n",
    "with open('./priors.ini') as f:\n",
    "    for line in f:\n",
    "        if('=' in line and '#' not in line and ';' not in line and 'a_planck' not in line):\n",
    "            line_priors=[]\n",
    "            pkeys.append(line.split(' ')[0])\n",
    "            pre_num_list = line[:-1].split('=')[1].split(' ')\n",
    "            for substr in pre_num_list: \n",
    "                if(is_number(substr)): \n",
    "                    line_priors.append(float(substr))\n",
    "            priors.append(line_priors)    \n",
    "            \n",
    "    \n",
    "\n",
    "init_values = dict(zip(vkeys,[v[1] for v in values]))\n",
    "ranges = dict(zip(vkeys,[[v[0],v[2]] for v in values]))\n",
    "\n",
    "init_mu = np.fromiter(init_values.values(),dtype=float)\n",
    "para_range = np.asarray([[v[0],v[2]] for v in values])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now for y3\n",
    "init_sig = (para_range[:, 1] - para_range[:, 0]) / 1000\n",
    "nParams = len(init_mu)\n",
    "print('parameters are', vkeys)\n",
    "idx_dict = dict(zip(vkeys,np.arange(len(values))))\n",
    "_nonlinear_indices = np.array([idx_dict['omega_m'],\n",
    "                               idx_dict['h0'],\n",
    "                               idx_dict['omega_b'],\n",
    "                               idx_dict['n_s'],\n",
    "                               idx_dict['A_s'],\n",
    "                               idx_dict['A1'],\n",
    "                               idx_dict['A2'],\n",
    "                               idx_dict['alpha1'],\n",
    "                               idx_dict['alpha2'],\n",
    "                               idx_dict['bias_ta']\n",
    "                              ])\n",
    "\n",
    "_constrained_indices = np.array([\n",
    "                                idx_dict['omega_m'],\n",
    "                                idx_dict['A_s'],\n",
    "                                idx_dict['A1'],\n",
    "                                idx_dict['A2'],\n",
    "                                idx_dict['alpha1'],\n",
    "                                idx_dict['alpha2'],\n",
    "                                idx_dict['bias_ta']\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nonlinear: ',np.asarray(vkeys)[_nonlinear_indices])\n",
    "print('constrained: ',np.asarray(vkeys)[_constrained_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosmosis pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the cosmosis pipeline.\n",
    "We only use cosmosis to compute the models (2pt functions),\n",
    "which are approximated by polynomial surrogates during sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3xpt readme instructions for running 2x2pt chain, picked a random one\n",
    "os.environ['DATAFILE'] = 'scale_cut_test_nl_bias_baryon.fits'\n",
    "os.environ['DEMODEL'] = 'lcdm'\n",
    "os.environ['SCALE_CUTS']='scales_2x2pt_12_6.ini'\n",
    "os.environ['DATASET']='2x2pt'\n",
    "os.environ['RUN_NAME_STR']='y3-test-bayesfast'\n",
    "os.environ['OUTDIR']='${SCRATCH}'+'/chains'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosmosis.runtime.config import Inifile\n",
    "from cosmosis.runtime.pipeline import LikelihoodPipeline\n",
    "import sys\n",
    "\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = open(os.devnull, 'w')\n",
    "ini_string='./2x2pt_fiducial/params.ini'\n",
    "ini = Inifile(ini_string)\n",
    "pipeline = LikelihoodPipeline(ini)\n",
    "sys.stdout = old_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pipeline.start_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipeline.run_results(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "from scipy.linalg import sqrtm\n",
    "\n",
    "#gaussian priors\n",
    "_prior_indices = np.array([idx_dict['bias_1'],\n",
    "                           idx_dict['bias_2'],\n",
    "                           idx_dict['bias_3'],\n",
    "                           idx_dict['bias_4'],\n",
    "                           idx_dict['bias_5'],\n",
    "                           idx_dict['m1'],\n",
    "                           idx_dict['m2'],\n",
    "                           idx_dict['m3'],\n",
    "                           idx_dict['m4'],\n",
    "                            ])\n",
    "\n",
    "_flat_indices = np.setdiff1d(np.fromiter(idx_dict.values(),dtype=int),_prior_indices)\n",
    "\n",
    "_prior_mu = np.asarray(priors)[:,0]\n",
    "_prior_sig = np.asarray(priors)[:,1]\n",
    "\n",
    "\n",
    "_prior_norm = (\n",
    "    -0.5 * np.sum(np.log(2 * np.pi * _prior_sig**2)) - np.sum(np.log(\n",
    "    norm.cdf(para_range[_prior_indices, 1], _prior_mu, _prior_sig) -\n",
    "    norm.cdf(para_range[_prior_indices, 0], _prior_mu, _prior_sig))) - \n",
    "    np.sum(np.log(para_range[_flat_indices,1] - para_range[_flat_indices,0])) \n",
    "              )\n",
    "\n",
    "_d = results.block['data_vector', '2pt_data']\n",
    "nData = _d.shape[0]\n",
    "print('nData',nData)\n",
    "\n",
    "#Note that this will not work if pm_marg=True\n",
    "_invC = results.block['data_vector', '2pt_inverse_covariance']\n",
    "\n",
    "_invC_r = sqrtm(_invC)\n",
    "\n",
    "_d_diag = _d @ _invC_r \n",
    "_norm = results.block['data_vector', '2pt_norm'] \n",
    "\n",
    "def des_prior_f(x): #prior chisq + log prior\n",
    "    chi2 = -0.5 * np.sum(((x[_prior_indices] - _prior_mu) / _prior_sig)**2)\n",
    "    return chi2 + _prior_norm\n",
    "\n",
    "def des_prior_j(x): #prior gradient\n",
    "    foo = np.zeros((1, nParams))\n",
    "    foo[0, _prior_indices] = -(x[_prior_indices] - _prior_mu) / _prior_sig**2\n",
    "    return foo\n",
    "\n",
    "def des_2pt_theory(x,_invC_r=_invC_r):\n",
    "    #run DES pipeline to get data*invCov , theory *invCov\n",
    "    try:\n",
    "        import os, sys\n",
    "        os.environ['OMP_NUM_THREADS'] = '1'\n",
    "        os.environ['DATAFILE'] = 'scale_cut_test_nl_bias_baryon.fits'\n",
    "        os.environ['DEMODEL'] = 'lcdm'\n",
    "        os.environ['SCALE_CUTS']='scales_2x2pt_12_6.ini'\n",
    "        os.environ['DATASET']='2x2pt'\n",
    "        os.environ['RUN_NAME_STR']='y3-test-bayesfast-broken'\n",
    "        os.environ['OUTDIR']='${SCRATCH}'+'/chains'\n",
    "        from cosmosis.runtime.config import Inifile\n",
    "        from cosmosis.runtime.pipeline import LikelihoodPipeline\n",
    "        ini_string='./2x2pt_fiducial/params.ini'\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        ini = Inifile(ini_string)\n",
    "        pipeline = LikelihoodPipeline(ini)\n",
    "        sys.stdout = old_stdout\n",
    "        res = pipeline.run_results(x)\n",
    "        rres = res.block['data_vector', '2pt_theory'] @ _invC_r\n",
    "        return rres\n",
    "    except:\n",
    "        print('Failed to run pipeline! Returning nans')\n",
    "        return np.nan*np.ones(nData)\n",
    "\n",
    "def chi2_f(m): #lhood chisq, uses covariance now\n",
    "    return np.atleast_1d(-0.5 * np.sum((m - _d_diag)**2) + _norm)\n",
    "\n",
    "def chi2_fj(m): #lood chisq gradient\n",
    "    return (np.atleast_1d(-0.5 * np.sum((m - _d_diag)**2) + _norm), \n",
    "            -(m - _d_diag)[np.newaxis])\n",
    "\n",
    "def des_post_f(like, x): #like+prior\n",
    "    return like + des_prior_f(x)\n",
    "\n",
    "def des_post_fj(like, x): #like + prior and prior gradients\n",
    "    return like + des_prior_f(x), np.concatenate(\n",
    "        (np.ones((1, 1)), des_prior_j(x)), axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BayesFast Setup:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BayesFast is not documented yet. Below is a brief note on its usage.\n",
    "Not all the functionality is used in this notebook.\n",
    "\n",
    "* `Module` : analogous to cosmosis modules, with optional analytic Jacobian. \n",
    "  * ```\n",
    "    __init__(self, fun=None, jac=None, fun_and_jac=None,\n",
    "             input_vars=['__var__'], output_vars=['__var__'],\n",
    "             copy_vars=None, paste_vars=None, delete_vars=None,\n",
    "             recombine_input=False, recombine_output=False,\n",
    "             var_scales=None, label=None, fun_args=(), fun_kwargs={},\n",
    "             jac_args=(), jac_kwargs={}, fun_and_jac_args=(),\n",
    "             fun_and_jac_kwargs={})\n",
    "    ```\n",
    "  * You may define its `fun` and/or `jac` and/or `fun_and_jac`,\n",
    "    and call them with `Module.fun` etc.\n",
    "    When `Module.fun` is called, we will first check if you have defined its `fun`.\n",
    "    If not, we will check if you have defined its `fun_and_jac`.\n",
    "    If still not, an exception will be raised. Similar for `Module.jac` and `Module.fun_and_jac`.\n",
    "  * You need to specify the name(s) of `input_vars` and `output_vars`\n",
    "    as a list of strings, or a string if there is only one variable.\n",
    "    This will be used to track the variables during the evaluation of the pipeline.\n",
    "    All the variables should be stored and used as 1-d numpy arraies.\n",
    "  * Let's say we have a `Module` with `input_vars` A and B, whose shapes are `(a,)` and `(b,)`.\n",
    "    While the `output_vars` are C and D, whose shapes are `(c,)` and `(d,)`.\n",
    "    Then the signature of its `fun` should be `(a,),(b,)->(c,),(d,)`.\n",
    "    The signature of its `jac` should be `(a,),(b,)->(c,a+b),(d,a+b)`.\n",
    "    The signature of its `fun_and_jac` should be `(a,),(b,)->((c,),(d,)),((c,a+b),(d,a+b))`.\n",
    "  * For convenience, you can also use the arguments `recombine_input` and `recombine_output`.\n",
    "    For the example above, if `recombine_input` is True, \n",
    "    the input of `fun` should have shape `(a+b,)`.\n",
    "    Assuming `a+b=e+f`, if `recombine_input` is `(e,f)`, \n",
    "    the input should have shape `(e,f)`. Similar for `recombine_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bayesfast as bf\n",
    "m_0 = bf.Module(fun=des_2pt_theory, input_vars='x', #parameters-> theory model\n",
    "                output_vars='m')\n",
    "m_1 = bf.Module(fun=chi2_f, fun_and_jac=chi2_fj, #theory model -> likelihood \n",
    "                input_vars='m', output_vars='like')\n",
    "m_2 = bf.Module(fun=des_post_f, fun_and_jac=des_post_fj, #likelihood and parameters -> log posterior\n",
    "                input_vars=['like', 'x'], output_vars='logp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Density`: derived from `Pipeline`, analogous to cosmosis LikelihoodPipeline.\n",
    "  * ```\n",
    "    __init__(self, module_list=None, input_vars=['__var__'], var_dims=None,\n",
    "             surrogate_list=None, var_scales=None, hard_bounds=False)\n",
    "    ```\n",
    "  * The overall input of `Density` should be a single array, \n",
    "    and you need to tell us how to split it using `input_vars` and `var_dims`.\n",
    "  * `var_scales` should have shape `(input_size, 2)`. \n",
    "    If None, it will be set as `((0,1),(0,1)...)`.\n",
    "  * If `hard_bounds` is False, \n",
    "    we only linearly rescale it by mapping `var_scales` to `((0,1),(0,1)...)`.\n",
    "    If `hard_bounds` is True,\n",
    "    we will transform it to `((-inf,+inf),(-inf,+inf)...)` using nonlinear transformation.\n",
    "    You can also set it separately for each variable \n",
    "    by using an array with shape `(input_size, 2)` for `hard_bounds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_0 = bf.Density(density_name='logp', module_list=[m_0, m_1, m_2], #stack modules to go from params -> log posterior\n",
    "                 input_vars='x', var_dims=nParams, var_scales=para_range, \n",
    "                 hard_bounds=True)\n",
    "d_0(start), results.post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `PolyConfig`: used to config `PolyModel`.\n",
    "  * ```\n",
    "    __init__(self, order, input_mask=None, output_mask=None, coef=None)\n",
    "    ```\n",
    "  * `order` should be one of `('linear', 'quadratic', 'cubic-2', 'cubic-3')`, \n",
    "    where `cubic-2` means cubic model without 'xyz' terms.\n",
    "  * If you only want to define it on some of the input (output) variables,\n",
    "    you can use `input_mask` (`output_mask`).\n",
    "\n",
    "\n",
    "* `Surrogate`: derived from `Module`.\n",
    "  * ```\n",
    "    __init__(self, input_size=None, output_size=None, scope=(0, 1),\n",
    "             input_vars=['__var__'], output_vars=['__var__'],\n",
    "             copy_vars=None, paste_vars=None, delete_vars=None,\n",
    "             recombine_input=True, *args, **kwargs)\n",
    "    ```\n",
    "  * `scope`: `(start,extent)`, e.g. `(0,1)` means it will replace #0 `Module` in `module_list`.\n",
    "\n",
    "\n",
    "* `PolyModel`: derived from `Surrogate`.\n",
    "  * ```\n",
    "    __init__(self, configs, *args, **kwargs)\n",
    "    ```\n",
    "  * `configs` should be a `PolyConfig` or a list of them. \n",
    "    Or you can also just use its `order` if you don't need to set the masks.\n",
    "    In this case, for example, `'quadratic'` will be interpreted as `('linear','quadratic')`.\n",
    "\n",
    "Here, during optimization, we use 24-d linear model.\n",
    "During sampling, we use 24-d linear plus 9-d quadratic model.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking size of polynomial inputs. There are {0} model paramters and {1} data points'.format(nParams,nData))\n",
    "s_0 = bf.modules.PolyModel('linear', nParams, nData, input_vars='x', #approximate theory model with linear model\n",
    "                           output_vars='m', var_scales=para_range)\n",
    "pc_0 = bf.modules.PolyConfig('linear') #another linear model\n",
    "pc_1 = bf.modules.PolyConfig('quadratic', input_mask=_nonlinear_indices) #nonlinear model - quadratic\n",
    "s_1 = bf.modules.PolyModel([pc_0, pc_1], nParams, nData, input_vars='x', #approximate theory model as linear + quadratic\n",
    "                           output_vars='m', var_scales=para_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`alpha_n=2` means we use twice the minimum number of samples required by fitting the surrogate model.\n",
    "\n",
    "We iterate the block quadratic model for two steps, and in the end, \n",
    "we use truncated importance sampling with n=2000 samples, while the weights w are truncated at < w >n^0.25.\n",
    "\n",
    "At the beginning, you need to provide a bunch of `x_0` to fit the initial surrogate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if in bounds provided by ranges in values.ini\n",
    "def _in_bound(xx, bound):\n",
    "    xxt = np.atleast_2d(xx).T\n",
    "    return np.product([np.where(xi>bound[i,0], True, False) * \n",
    "                       np.where(xi<bound[i,1], True, False) for i, xi in \n",
    "                       enumerate(xxt)], axis=0).astype(bool)\n",
    "\n",
    "\n",
    "#setting up steps in recipe\n",
    "opt_0 = bf.recipe.OptimizeStep(s_0, alpha_n=2,hmc_options={\"sampler_options\":{\"max_treedepth\":5}\n",
    "                                                           \"n_iter\":1000,\n",
    "                                                           \"n_warmup\":200}) #linear model for optimizer\n",
    "sam_0 = bf.recipe.SampleStep(s_1, alpha_n=2, reuse_steps=1,  #linear+quadratic sample step\n",
    "                             fit_options={'use_mu_f': True},sample_options={\"sampler_options\":{\"max_treedepth\":5},\n",
    "                                                                            \"n_iter\":1000,\n",
    "                                                                            \"n_warmup\":200})\n",
    "sam_1 = bf.recipe.SampleStep(s_1, alpha_n=2, reuse_steps=1, #linear+quadratic sample step\n",
    "                             fit_options={'use_mu_f': True},sample_options={\"sampler_options\":{\"max_treedepth\":8},\n",
    "                                                                            \"n_iter\":1000,\n",
    "                                                                            \"n_warmup\":200})\n",
    "#surrogate model training points\n",
    "x_0 = bf.utils.random.multivariate_normal(init_mu, np.diag(init_sig**2), 100)\n",
    "x_0 = x_0[_in_bound(x_0, para_range)] #checking that points are inside bounds\n",
    "\n",
    "if(not useIS):\n",
    "    r_0 = bf.recipe.Recipe(density=d_0, client=client, optimize=opt_0, \n",
    "                       sample=[sam_0, sam_1],\n",
    "                       x_0=x_0, \n",
    "                       random_state=1)\n",
    "else:\n",
    "    pos_0 = bf.recipe.PostStep(n_is=n_IS, k_trunc=0.25) #post step - importance sampling\n",
    "\n",
    "    r_0 = bf.recipe.Recipe(density=d_0, client=client, optimize=opt_0, #putting it all together\n",
    "                           sample=[sam_0, sam_1],\n",
    "                           post=pos_0, \n",
    "                           x_0=x_0, \n",
    "                           random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Bayesfast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.strftime('%H:%M%p %Z on %b %d, %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_0.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.strftime('%H:%M%p %Z on %b %d, %Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_0.get()._fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots - bayesfast linear+quadratic approx to posterior (with and without importance sampling)\n",
    "%matplotlib inline\n",
    "from getdist import plots, MCSamples\n",
    "import matplotlib.pyplot as plt\n",
    "labels = ['\\\\Omega_m', \n",
    "          'A_s', \n",
    "          'A_{1, \\\\rm IA}', \n",
    "          'A_{2, \\\\rm IA}',\n",
    "          '\\\\alpha_{1, \\\\rm IA}',\n",
    "          '\\\\alpha_{2, \\\\rm IA}',\n",
    "          'b_{\\\\rm ta}'\n",
    "         ] \n",
    "names = [\"x%s\"%i for i in range(len(labels))]\n",
    "n_CALL = r0.n_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(chain_supplied):\n",
    "    s_mn = MCSamples(\n",
    "        samples=chain_x_mn[:, _constrained_indices], weights=chain_p_mn, names=names, \n",
    "        labels=labels, ranges=dict(zip(names, para_range[_constrained_indices])), \n",
    "        label='MultiNest: original model ({0} iterations)'.format(chain_p_mn.shape[0]))\n",
    "    \n",
    "s_bf = MCSamples(\n",
    "    samples=r_0.get().samples[:, _constrained_indices], names=names, \n",
    "    labels=labels, ranges=dict(zip(names, para_range[_constrained_indices])), \n",
    "    label='BayesFast 5-5-8: block quadratic model ({0} calls)'.format(n_CALL-n_IS))\n",
    "\n",
    "s_bf_i = MCSamples(\n",
    "    samples=r_0.get().samples[:, _constrained_indices],\n",
    "    weights=r_0.get().weights, names=names, \n",
    "    labels=labels, ranges=dict(zip(names, para_range[_constrained_indices])), \n",
    "    label='BayesFast 5-5-8: block quadratic model with IS ({0} calls)'.format(n_CALL))\n",
    "g = plots.getSubplotPlotter()\n",
    "g.settings.figure_legend_loc = 'upper right'\n",
    "g.settings.axes_fontsize = 14\n",
    "g.settings.lab_fontsize = 16\n",
    "g.settings.legend_fontsize = 15\n",
    "g.settings.lw_contour = 2\n",
    "g.settings.lw1 = 2\n",
    "if(chain_supplied):\n",
    "    g.triangle_plot([ s_mn, s_bf, s_bf_i], filled=False, contour_args={'alpha':1}, \n",
    "                    diag1d_kwargs={'normalized':True}, contour_colors=[ 'dodgerblue',\n",
    "                    'gold', 'red'])\n",
    "else:\n",
    "    g.triangle_plot([s_bf, s_bf_i], filled=False, contour_args={'alpha':1}, \n",
    "                    diag1d_kwargs={'normalized':True}, contour_colors=['gold', 'red'])\n",
    "plt.tight_layout()\n",
    "#plt.savefig('y3_bf.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cosmosis-nbg",
   "language": "python",
   "name": "cosmosis-nbh"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
